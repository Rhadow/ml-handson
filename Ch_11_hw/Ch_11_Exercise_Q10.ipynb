{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/howard/anaconda3/envs/mlbook/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/howard/anaconda3/envs/mlbook/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden_neurons = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "\n",
    "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = tf.keras.datasets.mnist.load_data()\n",
    "X_train_raw = X_train_raw.astype(np.float32).reshape(-1, n_inputs) / 255\n",
    "X_test_raw = X_test_raw.astype(np.float32).reshape(-1, n_inputs) / 255\n",
    "y_train_raw = y_train_raw.astype(np.int32)\n",
    "y_test_raw = y_test_raw.astype(np.int32)\n",
    "\n",
    "X_train = X_train_raw[:55000]\n",
    "y_train = y_train_raw[:55000]\n",
    "X_val = X_train_raw[55000:]\n",
    "y_val = y_train_raw[55000:]\n",
    "\n",
    "def generate_comparison_training_batch(batch_size, images, labels):\n",
    "    size1 = batch_size // 2\n",
    "    size2 = batch_size - size1\n",
    "    if size1 != size2 and np.random.rand() > 0.5:\n",
    "        size1, size2 = size2, size1\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < size1:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([1])\n",
    "    while len(X) < batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if labels[rnd_idx1] != labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([0])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(X)[rnd_indices], np.array(y)[rnd_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "def dnn(inputs, n_layers=5, n_neurons=100, activation=tf.nn.elu, initializer=he_init, batch_norm_momentum=None, name=\"\", dropout_rate=None):\n",
    "    for layer in range(n_layers):\n",
    "        inputs = tf.layers.dense(inputs, n_neurons, kernel_initializer=initializer, name=\"%s_hidden%d\" % (name, layer + 1))\n",
    "        inputs = activation(inputs, name=\"%s_hidden%d_output\" % (name, layer + 1))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DNN to Detect Same Digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float64, (None, 2, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, [None, 1], name=\"y\")\n",
    "X_A, X_B = tf.unstack(X, axis=1)\n",
    "\n",
    "# DNN\n",
    "dnn_A = dnn(X_A, name=\"DNN_A\")\n",
    "dnn_B = dnn(X_B, name=\"DNN_B\")\n",
    "dnn_AB = tf.concat([dnn_A, dnn_B], 1)\n",
    "hidden_AB = tf.layers.dense(dnn_AB, 10, name=\"hidden_AB\", kernel_initializer=he_init, activation=tf.nn.elu)\n",
    "logits = tf.layers.dense(hidden_AB, 1, name=\"logits\", kernel_initializer=he_init)\n",
    "y_proba = tf.sigmoid(logits)\n",
    "\n",
    "# Loss\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y, tf.float64), logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# Train\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "# Eval\n",
    "y_pred = tf.cast(tf.greater_equal(y_proba, 0.5), tf.int32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y), tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1, y_test1 = generate_comparison_training_batch(len(X_test_raw), X_test_raw, y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]\t0\tLoss: 0.420595\n",
      "[TEST]\t0\tAccuracy: 79.63%\n",
      "[TRAIN]\t1\tLoss: 0.343084\n",
      "[TRAIN]\t2\tLoss: 0.289761\n",
      "[TRAIN]\t3\tLoss: 0.288865\n",
      "[TRAIN]\t4\tLoss: 0.221917\n",
      "[TRAIN]\t5\tLoss: 0.206437\n",
      "[TRAIN]\t6\tLoss: 0.187823\n",
      "[TRAIN]\t7\tLoss: 0.215581\n",
      "[TRAIN]\t8\tLoss: 0.153604\n",
      "[TRAIN]\t9\tLoss: 0.187798\n",
      "[TRAIN]\t10\tLoss: 0.176235\n",
      "[TEST]\t10\tAccuracy: 94.42%\n",
      "[TRAIN]\t11\tLoss: 0.120984\n",
      "[TRAIN]\t12\tLoss: 0.145719\n",
      "[TRAIN]\t13\tLoss: 0.135987\n",
      "[TRAIN]\t14\tLoss: 0.099689\n",
      "[TRAIN]\t15\tLoss: 0.105212\n",
      "[TRAIN]\t16\tLoss: 0.103685\n",
      "[TRAIN]\t17\tLoss: 0.090757\n",
      "[TRAIN]\t18\tLoss: 0.097564\n",
      "[TRAIN]\t19\tLoss: 0.076935\n",
      "[TRAIN]\t20\tLoss: 0.084564\n",
      "[TEST]\t20\tAccuracy: 96.18%\n",
      "[TRAIN]\t21\tLoss: 0.134126\n",
      "[TRAIN]\t22\tLoss: 0.076028\n",
      "[TRAIN]\t23\tLoss: 0.082661\n",
      "[TRAIN]\t24\tLoss: 0.099912\n",
      "[TRAIN]\t25\tLoss: 0.066036\n",
      "[TRAIN]\t26\tLoss: 0.082318\n",
      "[TRAIN]\t27\tLoss: 0.074621\n",
      "[TRAIN]\t28\tLoss: 0.048043\n",
      "[TRAIN]\t29\tLoss: 0.071715\n",
      "[TRAIN]\t30\tLoss: 0.094824\n",
      "[TEST]\t30\tAccuracy: 96.75%\n",
      "[TRAIN]\t31\tLoss: 0.072406\n",
      "[TRAIN]\t32\tLoss: 0.047032\n",
      "[TRAIN]\t33\tLoss: 0.088157\n",
      "[TRAIN]\t34\tLoss: 0.088646\n",
      "[TRAIN]\t35\tLoss: 0.045027\n",
      "[TRAIN]\t36\tLoss: 0.048577\n",
      "[TRAIN]\t37\tLoss: 0.048339\n",
      "[TRAIN]\t38\tLoss: 0.048941\n",
      "[TRAIN]\t39\tLoss: 0.066109\n",
      "[TRAIN]\t40\tLoss: 0.046810\n",
      "[TEST]\t40\tAccuracy: 97.38%\n",
      "[TRAIN]\t41\tLoss: 0.049687\n",
      "[TRAIN]\t42\tLoss: 0.063242\n",
      "[TRAIN]\t43\tLoss: 0.041871\n",
      "[TRAIN]\t44\tLoss: 0.046478\n",
      "[TRAIN]\t45\tLoss: 0.040237\n",
      "[TRAIN]\t46\tLoss: 0.048645\n",
      "[TRAIN]\t47\tLoss: 0.053320\n",
      "[TRAIN]\t48\tLoss: 0.035380\n",
      "[TRAIN]\t49\tLoss: 0.027024\n",
      "[TRAIN]\t50\tLoss: 0.052006\n",
      "[TEST]\t50\tAccuracy: 97.71%\n",
      "[TRAIN]\t51\tLoss: 0.034105\n",
      "[TRAIN]\t52\tLoss: 0.045368\n",
      "[TRAIN]\t53\tLoss: 0.044988\n",
      "[TRAIN]\t54\tLoss: 0.041250\n",
      "[TRAIN]\t55\tLoss: 0.022478\n",
      "[TRAIN]\t56\tLoss: 0.055965\n",
      "[TRAIN]\t57\tLoss: 0.040668\n",
      "[TRAIN]\t58\tLoss: 0.032151\n",
      "[TRAIN]\t59\tLoss: 0.011884\n",
      "[TRAIN]\t60\tLoss: 0.016232\n",
      "[TEST]\t60\tAccuracy: 97.53%\n",
      "[TRAIN]\t61\tLoss: 0.035939\n",
      "[TRAIN]\t62\tLoss: 0.038224\n",
      "[TRAIN]\t63\tLoss: 0.024284\n",
      "[TRAIN]\t64\tLoss: 0.008606\n",
      "[TRAIN]\t65\tLoss: 0.031258\n",
      "[TRAIN]\t66\tLoss: 0.017651\n",
      "[TRAIN]\t67\tLoss: 0.030564\n",
      "[TRAIN]\t68\tLoss: 0.033588\n",
      "[TRAIN]\t69\tLoss: 0.032050\n",
      "[TRAIN]\t70\tLoss: 0.017207\n",
      "[TEST]\t70\tAccuracy: 97.84%\n",
      "[TRAIN]\t71\tLoss: 0.020796\n",
      "[TRAIN]\t72\tLoss: 0.015243\n",
      "[TRAIN]\t73\tLoss: 0.010158\n",
      "[TRAIN]\t74\tLoss: 0.010161\n",
      "[TRAIN]\t75\tLoss: 0.032137\n",
      "[TRAIN]\t76\tLoss: 0.022446\n",
      "[TRAIN]\t77\tLoss: 0.025688\n",
      "[TRAIN]\t78\tLoss: 0.010945\n",
      "[TRAIN]\t79\tLoss: 0.017914\n",
      "[TRAIN]\t80\tLoss: 0.014886\n",
      "[TEST]\t80\tAccuracy: 98.01%\n",
      "[TRAIN]\t81\tLoss: 0.036800\n",
      "[TRAIN]\t82\tLoss: 0.022312\n",
      "[TRAIN]\t83\tLoss: 0.008184\n",
      "[TRAIN]\t84\tLoss: 0.010932\n",
      "[TRAIN]\t85\tLoss: 0.028264\n",
      "[TRAIN]\t86\tLoss: 0.050445\n",
      "[TRAIN]\t87\tLoss: 0.039509\n",
      "[TRAIN]\t88\tLoss: 0.015150\n",
      "[TRAIN]\t89\tLoss: 0.013447\n",
      "[TRAIN]\t90\tLoss: 0.020360\n",
      "[TEST]\t90\tAccuracy: 98.03%\n",
      "[TRAIN]\t91\tLoss: 0.011529\n",
      "[TRAIN]\t92\tLoss: 0.015658\n",
      "[TRAIN]\t93\tLoss: 0.016763\n",
      "[TRAIN]\t94\tLoss: 0.008636\n",
      "[TRAIN]\t95\tLoss: 0.042228\n",
      "[TRAIN]\t96\tLoss: 0.017657\n",
      "[TRAIN]\t97\tLoss: 0.004436\n",
      "[TRAIN]\t98\tLoss: 0.015967\n",
      "[TRAIN]\t99\tLoss: 0.017107\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(len(X_train) // batch_size):\n",
    "            X_batch, y_batch = generate_comparison_training_batch(batch_size, X_train, y_train)\n",
    "            loss_train, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        print(\"[TRAIN]\\t{}\\tLoss: {:.6f}\".format(epoch, loss_train))\n",
    "        if (epoch % 10 == 0):\n",
    "            accuracy_test = sess.run(accuracy, feed_dict={X: X_test1, y: y_test1})\n",
    "            print(\"[TEST]\\t{}\\tAccuracy: {:.2f}%\".format(epoch, accuracy_test * 100))\n",
    "        \n",
    "    saver.save(sess, \"./my_mnist_comparison_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, (None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, (None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"DNN_A\")\n",
    "# frozen_outputs = tf.stop_gradient(dnn_outputs)\n",
    "logits = tf.layers.dense(dnn_outputs, 10, name=\"cls_logits\", kernel_initializer=he_init)\n",
    "\n",
    "# Loss\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"cls_loss\")\n",
    "\n",
    "# Train\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss, name=\"cls_training_op\")\n",
    "\n",
    "# # Eval\n",
    "correct = tf.cast(tf.nn.in_top_k(logits, y, 1), tf.float32)\n",
    "accuracy = tf.reduce_mean(correct, name=\"cls_accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n",
    "# restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]\t0\tLoss: 0.448841\n",
      "[TEST]\t0\tAccuracy: 90.10%\n",
      "[TRAIN]\t1\tLoss: 0.321422\n",
      "[TRAIN]\t2\tLoss: 0.178332\n",
      "[TRAIN]\t3\tLoss: 0.089012\n",
      "[TRAIN]\t4\tLoss: 0.047456\n",
      "[TRAIN]\t5\tLoss: 0.080921\n",
      "[TRAIN]\t6\tLoss: 0.017963\n",
      "[TRAIN]\t7\tLoss: 0.017015\n",
      "[TRAIN]\t8\tLoss: 0.003149\n",
      "[TRAIN]\t9\tLoss: 0.004140\n",
      "[TRAIN]\t10\tLoss: 0.000383\n",
      "[TEST]\t10\tAccuracy: 93.72%\n",
      "[TRAIN]\t11\tLoss: 0.000930\n",
      "[TRAIN]\t12\tLoss: 0.000932\n",
      "[TRAIN]\t13\tLoss: 0.000502\n",
      "[TRAIN]\t14\tLoss: 0.000451\n",
      "[TRAIN]\t15\tLoss: 0.000121\n",
      "[TRAIN]\t16\tLoss: 0.000940\n",
      "[TRAIN]\t17\tLoss: 0.000351\n",
      "[TRAIN]\t18\tLoss: 0.000066\n",
      "[TRAIN]\t19\tLoss: 0.000634\n",
      "[TRAIN]\t20\tLoss: 0.000294\n",
      "[TEST]\t20\tAccuracy: 93.80%\n",
      "[TRAIN]\t21\tLoss: 0.000334\n",
      "[TRAIN]\t22\tLoss: 0.000304\n",
      "[TRAIN]\t23\tLoss: 0.000207\n",
      "[TRAIN]\t24\tLoss: 0.001060\n",
      "[TRAIN]\t25\tLoss: 0.000312\n",
      "[TRAIN]\t26\tLoss: 0.000253\n",
      "[TRAIN]\t27\tLoss: 0.000125\n",
      "[TRAIN]\t28\tLoss: 0.000095\n",
      "[TRAIN]\t29\tLoss: 0.000173\n",
      "[TRAIN]\t30\tLoss: 0.000109\n",
      "[TEST]\t30\tAccuracy: 93.85%\n",
      "[TRAIN]\t31\tLoss: 0.000267\n",
      "[TRAIN]\t32\tLoss: 0.000253\n",
      "[TRAIN]\t33\tLoss: 0.000095\n",
      "[TRAIN]\t34\tLoss: 0.000390\n",
      "[TRAIN]\t35\tLoss: 0.000040\n",
      "[TRAIN]\t36\tLoss: 0.000176\n",
      "[TRAIN]\t37\tLoss: 0.000168\n",
      "[TRAIN]\t38\tLoss: 0.000131\n",
      "[TRAIN]\t39\tLoss: 0.000096\n",
      "[TRAIN]\t40\tLoss: 0.000210\n",
      "[TEST]\t40\tAccuracy: 93.90%\n",
      "[TRAIN]\t41\tLoss: 0.000172\n",
      "[TRAIN]\t42\tLoss: 0.000138\n",
      "[TRAIN]\t43\tLoss: 0.000132\n",
      "[TRAIN]\t44\tLoss: 0.000066\n",
      "[TRAIN]\t45\tLoss: 0.000113\n",
      "[TRAIN]\t46\tLoss: 0.000138\n",
      "[TRAIN]\t47\tLoss: 0.000028\n",
      "[TRAIN]\t48\tLoss: 0.000191\n",
      "[TRAIN]\t49\tLoss: 0.000166\n",
      "[TRAIN]\t50\tLoss: 0.000121\n",
      "[TEST]\t50\tAccuracy: 93.95%\n",
      "[TRAIN]\t51\tLoss: 0.000030\n",
      "[TRAIN]\t52\tLoss: 0.000052\n",
      "[TRAIN]\t53\tLoss: 0.000056\n",
      "[TRAIN]\t54\tLoss: 0.000051\n",
      "[TRAIN]\t55\tLoss: 0.000018\n",
      "[TRAIN]\t56\tLoss: 0.000111\n",
      "[TRAIN]\t57\tLoss: 0.000072\n",
      "[TRAIN]\t58\tLoss: 0.000033\n",
      "[TRAIN]\t59\tLoss: 0.000043\n",
      "[TRAIN]\t60\tLoss: 0.000073\n",
      "[TEST]\t60\tAccuracy: 93.91%\n",
      "[TRAIN]\t61\tLoss: 0.000064\n",
      "[TRAIN]\t62\tLoss: 0.000189\n",
      "[TRAIN]\t63\tLoss: 0.000089\n",
      "[TRAIN]\t64\tLoss: 0.000095\n",
      "[TRAIN]\t65\tLoss: 0.000112\n",
      "[TRAIN]\t66\tLoss: 0.000068\n",
      "[TRAIN]\t67\tLoss: 0.000037\n",
      "[TRAIN]\t68\tLoss: 0.000095\n",
      "[TRAIN]\t69\tLoss: 0.000035\n",
      "[TRAIN]\t70\tLoss: 0.000009\n",
      "[TEST]\t70\tAccuracy: 93.90%\n",
      "[TRAIN]\t71\tLoss: 0.000080\n",
      "[TRAIN]\t72\tLoss: 0.000029\n",
      "[TRAIN]\t73\tLoss: 0.000020\n",
      "[TRAIN]\t74\tLoss: 0.000126\n",
      "[TRAIN]\t75\tLoss: 0.000138\n",
      "[TRAIN]\t76\tLoss: 0.000014\n",
      "[TRAIN]\t77\tLoss: 0.000049\n",
      "[TRAIN]\t78\tLoss: 0.000051\n",
      "[TRAIN]\t79\tLoss: 0.000074\n",
      "[TRAIN]\t80\tLoss: 0.000043\n",
      "[TEST]\t80\tAccuracy: 93.89%\n",
      "[TRAIN]\t81\tLoss: 0.000066\n",
      "[TRAIN]\t82\tLoss: 0.000055\n",
      "[TRAIN]\t83\tLoss: 0.000028\n",
      "[TRAIN]\t84\tLoss: 0.000086\n",
      "[TRAIN]\t85\tLoss: 0.000057\n",
      "[TRAIN]\t86\tLoss: 0.000067\n",
      "[TRAIN]\t87\tLoss: 0.000043\n",
      "[TRAIN]\t88\tLoss: 0.000063\n",
      "[TRAIN]\t89\tLoss: 0.000104\n",
      "[TRAIN]\t90\tLoss: 0.000052\n",
      "[TEST]\t90\tAccuracy: 93.89%\n",
      "[TRAIN]\t91\tLoss: 0.000044\n",
      "[TRAIN]\t92\tLoss: 0.000061\n",
      "[TRAIN]\t93\tLoss: 0.000101\n",
      "[TRAIN]\t94\tLoss: 0.000026\n",
      "[TRAIN]\t95\tLoss: 0.000037\n",
      "[TRAIN]\t96\tLoss: 0.000059\n",
      "[TRAIN]\t97\tLoss: 0.000091\n",
      "[TRAIN]\t98\tLoss: 0.000078\n",
      "[TRAIN]\t99\tLoss: 0.000056\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "#     restore_saver.restore(sess, \"./my_mnist_comparison_model.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_val))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_val) // batch_size):\n",
    "            X_batch, y_batch = X_val[rnd_indices], y_val[rnd_indices]\n",
    "            loss_train, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        print(\"[TRAIN]\\t{}\\tLoss: {:.6f}\".format(epoch, loss_train))\n",
    "        if (epoch % 10 == 0):\n",
    "            accuracy_test = sess.run(accuracy, feed_dict={X: X_test_raw, y: y_test_raw})\n",
    "            print(\"[TEST]\\t{}\\tAccuracy: {:.2f}%\".format(epoch, accuracy_test * 100))\n",
    "    new_saver.save(sess, \"./my_mnist_recognize_model.ckpt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
