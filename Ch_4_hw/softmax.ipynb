{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Batch Gradient Descent with early stopping for Softmax Regression without using Scikit-Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the libraries and the IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = iris.data[:, 2:] # Only get pedal width and height\n",
    "y_raw = iris.target\n",
    "\n",
    "X_with_bias = np.c_[np.ones((len(X_raw), 1)), X_raw];\n",
    "\n",
    "np.random.seed(3433)\n",
    "random_indexes = np.random.permutation(len(X_raw))\n",
    "\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "val_num = math.floor(val_ratio * len(X_raw))\n",
    "test_num = math.floor(test_ratio * len(X_raw))\n",
    "train_num = len(X_raw) - val_num - test_num\n",
    "\n",
    "X_train = X_with_bias[random_indexes[:train_num]]\n",
    "y_train = y_raw[random_indexes[:train_num]]\n",
    "X_val = X_with_bias[random_indexes[train_num:-test_num]]\n",
    "y_val = y_raw[random_indexes[train_num:-test_num]]\n",
    "X_test = X_with_bias[random_indexes[-test_num:]]\n",
    "y_test = y_raw[random_indexes[-test_num:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(arr):\n",
    "    classes = len(np.unique(arr))\n",
    "    result = np.zeros((len(arr), classes))\n",
    "    result[np.arange(len(arr)), arr] = 1\n",
    "    return result\n",
    "\n",
    "y_train_one_hot = one_hot(y_train)\n",
    "y_val_one_hot = one_hot(y_val)\n",
    "y_test_one_hot = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.780427841483177\n",
      "500 0.3669964205865573\n",
      "1000 0.27158782196900244\n",
      "1500 0.22385002416317776\n",
      "2000 0.19344530661380266\n",
      "2500 0.17189625674099207\n",
      "3000 0.15564732311116947\n",
      "3500 0.14287665308568848\n",
      "4000 0.13253236325315337\n",
      "4500 0.12395690182130371\n",
      "5000 0.11671505661683196\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy Cost Function\n",
    "def x_entropy(prob_matrix, expected_matrix, epsilon):\n",
    "    m = len(prob_matrix)\n",
    "    k = prob_matrix.shape[1]\n",
    "    total_error = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        tmp = 0\n",
    "        for j in range(k):\n",
    "            tmp += (expected_matrix[i][j] * np.log(prob_matrix[i][j] + epsilon))\n",
    "        total_error += tmp\n",
    "    return -total_error / m\n",
    "#     return -np.mean(np.sum(expected_matrix * np.log(prob_matrix + epsilon), axis=1))\n",
    "    \n",
    "    \n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits)\n",
    "    sum_logits = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    return exp_logits / sum_logits\n",
    "\n",
    "\n",
    "eta = 0.1\n",
    "iteration = 5001\n",
    "epsilon = 1e-7\n",
    "theta = np.random.randn(X_train.shape[1], len(np.unique(y_raw)))\n",
    "\n",
    "for i in range(iteration):\n",
    "    logit = X_train.dot(theta)\n",
    "    temp_predict = softmax(logit)\n",
    "    loss = x_entropy(temp_predict, y_train_one_hot, epsilon)\n",
    "    if (i % 500 == 0):\n",
    "        print(i, loss)\n",
    "    gradient = X_train.T.dot(temp_predict - y_train_one_hot) / len(X_train)\n",
    "    theta = theta - eta * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.62320594,  1.62993764, -8.91799214],\n",
       "       [-1.51330669,  0.22919687,  0.22075157],\n",
       "       [-4.8946616 , -1.50048214,  5.02367567]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "y_val_scores = softmax(X_val.dot(theta))\n",
    "y_val_predict = np.argmax(y_val_scores, axis = 1)\n",
    "accuracy_score = np.mean(y_val_predict == y_val)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Ridge Regularization and Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8185811819995337\n",
      "500 0.4720227487010162\n",
      "1000 0.4201609814645636\n",
      "1500 0.4000046231997925\n",
      "2000 0.3900474991943235\n",
      "2500 0.38467322237533774\n",
      "3000 0.3816325829880317\n",
      "3500 0.3798606486563636\n",
      "4000 0.3788070488076408\n",
      "4500 0.37817150295300495\n",
      "5000 0.37778405943908144\n",
      "5500 0.37754598498418634\n",
      "6000 0.3773988093217512\n",
      "6500 0.3773074039728006\n",
      "7000 0.3772504314485231\n",
      "7500 0.3772148213776322\n",
      "8000 0.377192514979669\n",
      "8500 0.377178518105095\n",
      "9000 0.37716972344690225\n",
      "Early stopping!\n",
      "Last Run:  9349 0.37716560144117117\n",
      "Current Run:  9350 0.3771655914413883\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "iteration = 35001\n",
    "epsilon = 1e-7\n",
    "theta = np.random.randn(X_train.shape[1], len(np.unique(y_raw)))\n",
    "alpha = 0.05\n",
    "last_loss = np.infty\n",
    "early_stop_diff = 1e-8\n",
    "\n",
    "for i in range(iteration):\n",
    "    logit = X_train.dot(theta)\n",
    "    temp_predict = softmax(logit)\n",
    "    l2_loss = 0.5 * np.sum(np.square(theta[1:]))\n",
    "    loss = -np.mean(np.sum(y_train_one_hot * np.log(temp_predict + epsilon), axis=1)) + alpha * l2_loss\n",
    "    if (i % 500 == 0):\n",
    "        print(i, loss)\n",
    "    \n",
    "    if last_loss - loss >= early_stop_diff:\n",
    "        last_loss = loss\n",
    "    else:\n",
    "        print('Early stopping!')\n",
    "        print('Last Run: ', i - 1, last_loss)\n",
    "        print('Current Run: ', i, loss)\n",
    "        break\n",
    "    gradient = X_train.T.dot(temp_predict - y_train_one_hot) / len(X_train) + np.r_[np.zeros([1, len(np.unique(y_raw))]), alpha * theta[1:]]\n",
    "    theta = theta - eta * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "y_val_scores = softmax(X_val.dot(theta))\n",
    "y_val_predict = np.argmax(y_val_scores, axis = 1)\n",
    "accuracy_score = np.mean(y_val_predict == y_val)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Result on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_test_scores = softmax(X_test.dot(theta))\n",
    "y_test_predict = np.argmax(y_test_scores, axis = 1)\n",
    "accuracy_score = np.mean(y_test_predict == y_test)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
